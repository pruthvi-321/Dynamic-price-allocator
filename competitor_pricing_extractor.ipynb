{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5294cc-ca4a-4883-81c9-e52ada96ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef94f077-bd49-465f-9859-adc39b827778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inputs set: {'product_id': 'SKU123', 'product_name': 'Dettol Liquid Handwash Refill 750ml', 'brand': 'Dettol', 'location': 'Bidar, KA, IN', 'cost_price': 92.0, 'min_margin_pct': 15.0, 'target_position': 'within_top3', 'beat_pct': 2.0, 'channels_to_consider': ['amazon', 'blinkit', 'flipkart', 'bigbasket', 'jiomart', 'dmart', 'zepto', 'swiggy', 'myntra', 'netmeds', 'apollo'], 'tax_rate_pct': 18.0, 'currency': 'INR', 'serpapi_key': '63d53fa8c12d000709d108a3f6be1a8f3bbf3b42', 'manual_urls': []}\n",
      "Runs folder: C:\\Users\\pruth\\Desktop\\MBA\\Paper\\ml-notebooks\\ml-notebooks\\pruthvi_project\\runs\n"
     ]
    }
   ],
   "source": [
    "# --- Headless Cell 1 (no widgets): edit variables below, then run once ---\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# === EDIT THESE VALUES ===\n",
    "PID          = \"SKU123\"\n",
    "PRODUCT_NAME = \"Dettol Liquid Handwash Refill 750ml\"\n",
    "BRAND        = \"Dettol\"\n",
    "LOCATION     = \"Bidar, KA, IN\"\n",
    "COST_PRICE   = 92.0\n",
    "MIN_MARGIN   = 15.0\n",
    "STRATEGY     = \"within_top3\"    # within_top3 | match_lowest | beat_lowest_by_pct | margin_first\n",
    "BEAT_PCT     = 2.0\n",
    "CHANNELS     = [\"amazon\",\"blinkit\",\"flipkart\",\"bigbasket\",\"jiomart\",\"dmart\",\"zepto\",\"swiggy\",\"myntra\",\"netmeds\",\"apollo\"]\n",
    "TAX_PCT      = 18.0\n",
    "CURRENCY     = \"INR\"\n",
    "SERPER_KEY   = \"63d53fa8c12d000709d108a3f6be1a8f3bbf3b42\"   # <-- put your Serper.dev key here\n",
    "MANUAL_URLS  = [\n",
    "    # Optional: paste a few product page URLs (PDP) to guarantee coverage\n",
    "    # \"https://www.amazon.in/dp/XXXXXXXXXX\",\n",
    "    # \"https://www.flipkart.com/p/itmXXXXXXXX\",\n",
    "]\n",
    "\n",
    "# === DO NOT EDIT BELOW ===\n",
    "USER_INPUT = dict(\n",
    "    product_id=PID,\n",
    "    product_name=PRODUCT_NAME,\n",
    "    brand=BRAND,\n",
    "    location=LOCATION,\n",
    "    cost_price=float(COST_PRICE),\n",
    "    min_margin_pct=float(MIN_MARGIN),\n",
    "    target_position=STRATEGY,\n",
    "    beat_pct=float(BEAT_PCT),\n",
    "    channels_to_consider=list(CHANNELS),\n",
    "    tax_rate_pct=float(TAX_PCT),\n",
    "    currency=CURRENCY,\n",
    "    serpapi_key=(SERPER_KEY or None),\n",
    "    manual_urls=list(MANUAL_URLS),\n",
    ")\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "RUNS_DIR = BASE_DIR / \"runs\"\n",
    "RUNS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"✅ Inputs set:\", USER_INPUT)\n",
    "print(\"Runs folder:\", RUNS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc05d8-0ad1-48a8-b823-6551186675af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] amazon: 310.0 @ https://www.amazon.in/Dettol-Original-Liquid-Soap-Refill-Original/dp/B07B32NXFR\n",
      "[OK] amazon: 380.0 @ https://www.amazon.in/Dettol-Liquid-Handwash-Original-Rupees/dp/B01N4I40K5\n",
      "[OK] amazon: 238.0 @ https://www.amazon.in/Dettol-Protection-ph-Balanced-Handwash-Skincare/dp/B07KVXZ4K5\n",
      "[OK] amazon: 285.0 @ https://www.amazon.in/Dettol-Original-Liquid-Hand-Refill/dp/B08537MS3D\n",
      "[OK] amazon: 269.0 @ https://www.amazon.in/Dettol-Antiseptic-Disinfection-Personal-Handwash/dp/B0BP5KXFJW\n",
      "[OK] flipkart: 192.0 @ https://www.flipkart.com/dettol-original-liquid-handwash-pouch-750ml-pack-2-hand-wash-pouch/p/itme217c228a6c62\n",
      "[OK] flipkart: 154.0 @ https://www.flipkart.com/dettol-liquid-handwash-refill-pouch-original-germ-protection-hand-wash/p/itmbffadfc5baf97\n",
      "[OK] flipkart: 218.0 @ https://www.flipkart.com/dettol-skincare-liquid-hand-wash-refill-pouch/p/itm1e763103b9dc1\n",
      "[skip] https://www.flipkart.com/dettol-aloe-coconut-foaming-handwash-dispenser-refill-hand-wash/p/itm47719a38caf5c\n",
      "[skip] https://www.flipkart.com/dettol-original-handwash-combo-offer-pack-2-hand-wash-refill-pouch/p/itmc391e19072168\n",
      "[WARN] HTTP 403 for https://www.bigbasket.com/pd/40034580/dettol-liquid-handwash-original-everyday-protection-fights-germs-750-ml/\n",
      "[skip] https://www.bigbasket.com/pd/40034580/dettol-liquid-handwash-original-everyday-protection-fights-germs-750-ml/\n",
      "[WARN] HTTP 403 for https://www.bigbasket.com/pd/40034579/dettol-liquid-handwash-skincare-everyday-protection-ph-balanced-moisturising-750-ml-refill/\n",
      "[skip] https://www.bigbasket.com/pd/40034579/dettol-liquid-handwash-skincare-everyday-protection-ph-balanced-moisturising-750-ml-refill/\n",
      "[WARN] HTTP 403 for https://www.jiomart.com/p/groceries/dettol-original-handwash-refill-750-ml/491231083\n",
      "[skip] https://www.jiomart.com/p/groceries/dettol-original-handwash-refill-750-ml/491231083\n",
      "[WARN] HTTP 403 for https://www.jiomart.com/p/groceries/dettol-liquid-handwash-refill-original-germ-protection-hand-wash-antibacterial-formula-original-germ-protection-handwash-liquid-soap-refill-750ml-pack-of-2/602149208\n",
      "[skip] https://www.jiomart.com/p/groceries/dettol-liquid-handwash-refill-original-germ-protection-hand-wash-antibacterial-formula-original-germ-protection-handwash-liquid-soap-refill-750ml-pack-of-2/602149208\n",
      "[WARN] HTTP 403 for https://www.jiomart.com/p/groceries/dettol-skincare-ph-balanced-liquid-handwash-refill-750-ml/491281198\n",
      "[skip] https://www.jiomart.com/p/groceries/dettol-skincare-ph-balanced-liquid-handwash-refill-750-ml/491281198\n",
      "[WARN] HTTP 403 for https://www.jiomart.com/p/groceries/dettol-original-liquid-handwash-750-ml-buy-1-get-1-free/590040983\n",
      "[skip] https://www.jiomart.com/p/groceries/dettol-original-liquid-handwash-750-ml-buy-1-get-1-free/590040983\n",
      "[WARN] HTTP 403 for https://www.jiomart.com/p/groceries/dettol-original-hand-wash-750-ml-single/603640195\n",
      "[skip] https://www.jiomart.com/p/groceries/dettol-original-hand-wash-750-ml-single/603640195\n",
      "[WARN] HTTP 403 for https://www.jiomart.com/p/beauty/dettol-liquid-handwash-refill-original-750-ml/607661643\n",
      "[skip] https://www.jiomart.com/p/beauty/dettol-liquid-handwash-refill-original-750-ml/607661643\n",
      "[skip] https://www.dmart.in/product/dettol-liquid-handwash-refill-pdoublebedsheet0dett3xx121119\n",
      "[skip] https://www.dmart.in/product/margo-vitamin-e-moisturisers-neem-soap-pbathsoaps1xx180725?selectedProd=1768553\n",
      "[skip] https://www.dmart.in/product/dettol-antiseptic-liquid-pantiseptic0dett1xx170222\n",
      "[SKIP robots] https://www.swiggy.com/stores/instamart/p/dettol-skincare-ph-balanced-hand-wash-refill-DL15YTGUS8?locId=MTUuNDc1MDY5NjgsNzMuODI3MDcyNjkw\n",
      "[skip] https://www.swiggy.com/stores/instamart/p/dettol-skincare-ph-balanced-hand-wash-refill-DL15YTGUS8?locId=MTUuNDc1MDY5NjgsNzMuODI3MDcyNjkw\n",
      "[SKIP robots] https://www.swiggy.com/stores/instamart/p/dettol-original-germ-protection-ph-balanced-liquid-handwash-refill-8085GUFK11?storeId=1381971\n",
      "[skip] https://www.swiggy.com/stores/instamart/p/dettol-original-germ-protection-ph-balanced-liquid-handwash-refill-8085GUFK11?storeId=1381971\n",
      "[SKIP robots] https://www.swiggy.com/instamart/p/dettol-original-hand-wash-germ-protection-handwash-refill-C9X4CJGLB1\n",
      "[skip] https://www.swiggy.com/instamart/p/dettol-original-hand-wash-germ-protection-handwash-refill-C9X4CJGLB1\n",
      "[SKIP robots] https://www.swiggy.com/instamart/p/dettol-antiseptic-liquid-for-first-aid-surface-disinfection-and-personal-hygiene-5UO9JZLMFO\n",
      "[skip] https://www.swiggy.com/instamart/p/dettol-antiseptic-liquid-for-first-aid-surface-disinfection-and-personal-hygiene-5UO9JZLMFO\n",
      "[SKIP robots] https://www.swiggy.com/instamart/p/dettol-original-germ-protection-ph-balanced-liquid-handwash-refill-8085GUFK11\n",
      "[skip] https://www.swiggy.com/instamart/p/dettol-original-germ-protection-ph-balanced-liquid-handwash-refill-8085GUFK11\n",
      "[SKIP robots] https://www.swiggy.com/instamart/city/pondicherry/b/dettol\n",
      "[skip] https://www.swiggy.com/instamart/city/pondicherry/b/dettol\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Scrape & build offers CSV for THIS product (Serper.dev + strict filters + fallbacks) ---\n",
    "# If needed first time:  !pip install requests beautifulsoup4 -q\n",
    "\n",
    "# Guard: ensure USER_INPUT exists & has required keys\n",
    "required_keys = {\n",
    "    \"product_id\",\"product_name\",\"brand\",\"location\",\"cost_price\",\"min_margin_pct\",\n",
    "    \"target_position\",\"beat_pct\",\"channels_to_consider\",\"tax_rate_pct\",\"currency\",\n",
    "    \"serpapi_key\",\"manual_urls\"\n",
    "}\n",
    "missing = [k for k in required_keys if k not in USER_INPUT]\n",
    "if missing:\n",
    "    raise ValueError(f\"USER_INPUT is missing keys: {missing}. Run Cell 1 or set USER_INPUT first.\")\n",
    "\n",
    "import re, time, json, html, math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Serper.dev key (we reuse USER_INPUT['serpapi_key'])\n",
    "SERPER_KEY = (USER_INPUT.get(\"serpapi_key\") or \"PASTE_SERPER_KEY_HERE\").strip()\n",
    "\n",
    "# Target sites\n",
    "TARGET_SITES = {\n",
    "    \"amazon\":   \"amazon.in\",\n",
    "    \"blinkit\":  \"blinkit.com\",\n",
    "    \"flipkart\": \"flipkart.com\",\n",
    "    \"bigbasket\":\"bigbasket.com\",\n",
    "    \"jiomart\":  \"jiomart.com\",\n",
    "    \"dmart\":    \"dmart.in\",\n",
    "    \"zepto\":    \"zeptonow.com\",\n",
    "    \"swiggy\":   \"swiggy.com\",\n",
    "    \"myntra\":   \"myntra.com\",\n",
    "    \"netmeds\":  \"netmeds.com\",\n",
    "    \"apollo\":   \"apollopharmacy.in\",\n",
    "}\n",
    "\n",
    "# Per-site product URL patterns (whitelist)\n",
    "SITE_PRODUCT_PATTERNS = {\n",
    "    \"amazon\":   [r\"/dp/\", r\"/gp/product/\"],\n",
    "    \"flipkart\": [r\"/p/\"],\n",
    "    \"bigbasket\":[r\"/pd/\"],\n",
    "    \"jiomart\":  [r\"/p/\"],\n",
    "    \"dmart\":    [r\"/product/\"],\n",
    "    \"myntra\":   [r\"/buy/\", r\"/\\d+/(?!reviews)\"],\n",
    "    \"netmeds\":  [r\"/non-prescriptions/\"],\n",
    "    \"apollo\":   [r\"/p/\"],\n",
    "    \"blinkit\":  [r\"/product/\"],\n",
    "    \"zepto\":    [r\"/product/\"],\n",
    "    \"swiggy\":   [r\"/instamart\", r\"/stores/\"],\n",
    "}\n",
    "\n",
    "# URL substrings to reject (non-PDP)\n",
    "REJECT_URL_SUBSTRINGS = [\n",
    "    \"/product-reviews\", \"/reviews\", \"/category\", \"/categories\",\n",
    "    \"/offers\", \"/deal\", \"/deals\", \"/search\", \"/s?k=\", \"/gp/help\",\n",
    "    \"/help\", \"/terms\", \"/privacy\", \"customer-reviews\", \"feedback\",\n",
    "    \"listing\", \"filters\", \"page=\",\n",
    "]\n",
    "\n",
    "# Bundle/Combo words to avoid\n",
    "BUNDLE_WORDS = [\"pack of\", \"packof\", \"x2\", \"x3\", \"combo\", \"buy1get1\", \"b1g1\", \"with pump\", \"refill + pump\", \"bundle\"]\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def sleep_secs(min_s=1.0, max_s=2.0):\n",
    "    import random; time.sleep(random.uniform(min_s, max_s))\n",
    "\n",
    "def can_fetch(url: str, user_agent=\"Mozilla/5.0\") -> bool:\n",
    "    try:\n",
    "        u = urlparse(url)\n",
    "        robots_url = f\"{u.scheme}://{u.netloc}/robots.txt\"\n",
    "        rp = RobotFileParser()\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        return rp.can_fetch(user_agent, url)\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def fetch(url: str, timeout=18, headers=None):\n",
    "    # stronger headers and a small retry loop reduce random 403/429s\n",
    "    base_headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.7\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"Pragma\": \"no-cache\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "    }\n",
    "    if headers: base_headers.update(headers)\n",
    "\n",
    "    if not can_fetch(url, base_headers.get(\"User-Agent\")):\n",
    "        print(f\"[SKIP robots] {url}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with requests.Session() as s:\n",
    "            s.headers.update(base_headers)\n",
    "            last = None\n",
    "            for _ in range(2):   # up to 2 tries\n",
    "                r = s.get(url, timeout=timeout)\n",
    "                last = r\n",
    "                ct = r.headers.get(\"Content-Type\",\"\")\n",
    "                if r.status_code == 200 and \"text/html\" in ct:\n",
    "                    return r.text\n",
    "                if r.status_code in (429, 503):\n",
    "                    time.sleep(1.2)\n",
    "            print(f\"[WARN] HTTP {last.status_code} for {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] fetch error for {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def source_from_url(url: str) -> str:\n",
    "    host = urlparse(url).netloc.lower().replace(\"www.\", \"\")\n",
    "    for key, dom in TARGET_SITES.items(): \n",
    "        if dom in host: return key\n",
    "    parts = host.split(\".\")\n",
    "    if len(parts) >= 3 and \".\".join(parts[-2:]) in {\"co.in\",\"com.au\",\"co.uk\"}: return parts[-3]\n",
    "    if len(parts) >= 2: return parts[-2]\n",
    "    return host\n",
    "\n",
    "def is_rejected_url(url: str) -> bool:\n",
    "    u = url.lower()\n",
    "    return any(s in u for s in REJECT_URL_SUBSTRINGS)\n",
    "\n",
    "def matches_site_product_rules(url: str, source: str) -> bool:\n",
    "    pats = SITE_PRODUCT_PATTERNS.get(source, [])\n",
    "    return True if not pats else any(re.search(p, url) for p in pats)\n",
    "\n",
    "def normalized(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").lower()).strip()\n",
    "\n",
    "def tokens_from_product(product_name: str, brand: str) -> list[str]:\n",
    "    base = normalized(product_name); toks = set()\n",
    "    for t in re.split(r\"[^a-z0-9]+\", base):\n",
    "        if t: toks.add(t)\n",
    "    if brand:\n",
    "        for t in re.split(r\"[^a-z0-9]+\", normalized(brand)):\n",
    "            if t: toks.add(t)\n",
    "    keep = set()\n",
    "    for t in toks:\n",
    "        if t in {\"dettol\",\"handwash\",\"refill\",\"liquid\",\"original\",\"skincare\",\"germ\",\"protection\"}: keep.add(t)\n",
    "        if re.fullmatch(r\"\\d{2,4}\", t): keep.add(t)      # e.g., 750\n",
    "        if t in {\"ml\",\"g\",\"gm\",\"gram\",\"litre\",\"l\"}: keep.add(t)\n",
    "    return list(keep) or list(toks)\n",
    "\n",
    "def likely_bundle(title: str) -> bool:\n",
    "    t = normalized(title)\n",
    "    return any(w in t for w in BUNDLE_WORDS) or bool(re.search(r\"\\b(pack\\s*of\\s*\\d+|\\d+\\s*x\\s*\\d+)\\b\", t))\n",
    "\n",
    "# Price extraction\n",
    "PRICE_PAT = re.compile(r\"(?:₹|INR[\\s\\.]?)\\s?([0-9]{1,3}(?:[, ]?[0-9]{2,3})*(?:\\.[0-9]+)?)\", re.I)\n",
    "\n",
    "def parse_possible_prices(text: str):\n",
    "    vals = []\n",
    "    for m in PRICE_PAT.finditer(text.replace(\"\\xa0\", \" \")):\n",
    "        num = m.group(1).replace(\",\", \"\").replace(\" \", \"\")\n",
    "        try: vals.append(float(num))\n",
    "        except: pass\n",
    "    return vals\n",
    "\n",
    "def parse_ldjson_price(soup: BeautifulSoup):\n",
    "    for tag in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        try: data = json.loads(tag.string or \"\")\n",
    "        except Exception: continue\n",
    "        candidates = data if isinstance(data, list) else [data]\n",
    "        for d in candidates:\n",
    "            if not isinstance(d, dict): continue\n",
    "            if (d.get(\"@type\") == \"Product\") or (\"offers\" in d):\n",
    "                offer = d.get(\"offers\", {})\n",
    "                if isinstance(offer, list) and offer: offer = offer[0]\n",
    "                price = offer.get(\"price\") if isinstance(offer, dict) else None\n",
    "                if price:\n",
    "                    try: return float(str(price).replace(\",\", \"\"))\n",
    "                    except: pass\n",
    "                p2 = d.get(\"price\") or d.get(\"priceSpecification\", {}).get(\"price\")\n",
    "                if p2:\n",
    "                    try: return float(str(p2).replace(\",\", \"\"))\n",
    "                    except: pass\n",
    "    return None\n",
    "\n",
    "def first_reasonable_price(soup: BeautifulSoup, price_min=10, price_max=100000):\n",
    "    p = parse_ldjson_price(soup)\n",
    "    if p and price_min <= p <= price_max: return p\n",
    "    texts = soup.get_text(\" \", strip=True)\n",
    "    candidates = [v for v in parse_possible_prices(texts) if price_min <= v <= price_max]\n",
    "    if not candidates: return None\n",
    "    candidates.sort()\n",
    "    return candidates[len(candidates)//2]\n",
    "\n",
    "# Legitimacy & math helpers\n",
    "def _to_float(x, default=0.0):\n",
    "    try:\n",
    "        if x is None or x == \"\": return default\n",
    "        return float(str(x).replace(\",\", \"\"))\n",
    "    except Exception: return default\n",
    "\n",
    "def _to_int(x, default=0):\n",
    "    try:\n",
    "        if x is None or x == \"\": return default\n",
    "        return int(str(x).replace(\",\", \"\"))\n",
    "    except Exception: return default\n",
    "\n",
    "def legitimacy_heuristics(row: dict) -> int:\n",
    "    score = 0\n",
    "    known = set(TARGET_SITES.keys())\n",
    "    if row.get(\"source\") in known: score += 20\n",
    "    try:\n",
    "        if urlparse(row.get(\"url\",\"\")).scheme == \"https\": score += 5\n",
    "    except Exception: pass\n",
    "    if row.get(\"source\") in known: score += 15\n",
    "    rating = _to_float(row.get(\"rating\"), 0.0)\n",
    "    rating_count = _to_int(row.get(\"rating_count\"), 0)\n",
    "    if rating_count >= 100 and rating >= 4.0: score += 10\n",
    "    if bool(row.get(\"in_stock\", True)): score += 10\n",
    "    if row.get(\"has_policy_pages\", True): score += 5\n",
    "    return int(score)\n",
    "\n",
    "def check_delivery(source: str, location: str, pincode: str = \"\") -> bool:\n",
    "    if source.lower() == \"dmart\" and \"bidar\" in (location or \"\").lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def comparable_price(row) -> float:\n",
    "    base = _to_float(row.get(\"base_price\"), 0.0)\n",
    "    shipping = _to_float(row.get(\"shipping_fee\"), 0.0)\n",
    "    cod = _to_float(row.get(\"cod_fee\"), 0.0)\n",
    "    coupon = _to_float(row.get(\"coupon_value\"), 0.0)\n",
    "    return max(base + shipping + cod - coupon, 0.0)\n",
    "\n",
    "# -------- Serper search (site-restricted) --------\n",
    "def serper_site_search(query: str, site_domain: str, api_key: str, num: int = 5):\n",
    "    if not api_key or api_key == \"PASTE_SERPER_KEY_HERE\":\n",
    "        print(\"⚠️ No Serper key set. Fill SERPAPI_KEY in Cell 1 or paste it above.\")\n",
    "        return []\n",
    "    try:\n",
    "        headers = {\"X-API-KEY\": api_key, \"Content-Type\": \"application/json\"}\n",
    "        payload = {\"q\": f\"site:{site_domain} {query}\", \"num\": num, \"gl\": \"in\", \"hl\": \"en\"}\n",
    "        r = requests.post(\"https://google.serper.dev/search\", json=payload, headers=headers, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        urls = []\n",
    "        for item in data.get(\"organic\", []):\n",
    "            link = item.get(\"link\")\n",
    "            if link: urls.append(link)\n",
    "        for item in data.get(\"shopping\", []):\n",
    "            link = item.get(\"link\")\n",
    "            if link: urls.append(link)\n",
    "        return list(dict.fromkeys(urls))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] serper error ({site_domain}):\", e)\n",
    "        return []\n",
    "\n",
    "# -------- Serper Shopping fallback (domain -> (price, url)) --------\n",
    "def serper_quick_prices(query: str, api_key: str) -> dict:\n",
    "    out = {}\n",
    "    if not api_key or api_key == \"PASTE_SERPER_KEY_HERE\":\n",
    "        return out\n",
    "    try:\n",
    "        headers = {\"X-API-KEY\": api_key, \"Content-Type\": \"application/json\"}\n",
    "        payload = {\"q\": query, \"num\": 10, \"gl\": \"in\", \"hl\": \"en\"}\n",
    "        r = requests.post(\"https://google.serper.dev/search\", json=payload, headers=headers, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        for item in data.get(\"shopping\", []):\n",
    "            link = item.get(\"link\"); price_str = item.get(\"price\")\n",
    "            if not link or not price_str: continue\n",
    "            m = re.search(r\"([\\d,]+(?:\\.\\d+)?)\", price_str)\n",
    "            if not m: continue\n",
    "            price = float(m.group(1).replace(\",\", \"\"))\n",
    "            dom = urlparse(link).netloc.lower().replace(\"www.\", \"\")\n",
    "            out[dom] = (price, link)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] serper_quick_prices error:\", e)\n",
    "        return out\n",
    "\n",
    "# -------- Scrape a single product page --------\n",
    "def scrape_product_page(url: str, product_name: str, strict_tokens: list[str], price_bounds=(10, 10000)):\n",
    "    html_text = fetch(url)\n",
    "    if not html_text:\n",
    "        return None\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "    title = soup.title.get_text(strip=True) if soup.title else url\n",
    "    if is_rejected_url(url) or likely_bundle(title):\n",
    "        return None\n",
    "\n",
    "    # Loosened: require at least 2 token matches (brand + size/category)\n",
    "    norm_title = normalized(title)\n",
    "    match_count = sum(1 for t in strict_tokens if t in norm_title)\n",
    "    if match_count < 2:\n",
    "        return None\n",
    "\n",
    "    price = first_reasonable_price(soup, price_min=price_bounds[0], price_max=price_bounds[1])\n",
    "    if not price:\n",
    "        return None\n",
    "    if not (price_bounds[0] <= price <= price_bounds[1]):\n",
    "        return None\n",
    "\n",
    "    text = soup.get_text(\" \", strip=True).lower()\n",
    "    in_stock = (\"out of stock\" not in text) and (\"unavailable\" not in text)\n",
    "\n",
    "    rating = None; rating_count = 0\n",
    "    m = re.search(r\"([0-5](?:\\.[0-9])?)\\s*[★⭐]\", text)\n",
    "    if m:\n",
    "        try: rating = float(m.group(1))\n",
    "        except: pass\n",
    "    m2 = re.search(r\"([1-9][0-9]{1,5})\\s*ratings\", text)\n",
    "    if m2:\n",
    "        try: rating_count = int(m2.group(1))\n",
    "        except: pass\n",
    "\n",
    "    row = dict(\n",
    "        source=source_from_url(url),\n",
    "        product_title=\" \".join(html.unescape(title or \"\").split())[:180],\n",
    "        base_price=float(price),\n",
    "        shipping_fee=0.0,\n",
    "        cod_fee=0.0,\n",
    "        coupon_value=0.0,\n",
    "        in_stock=bool(in_stock),\n",
    "        url=url,\n",
    "        timestamp=datetime.now(timezone.utc).isoformat(),\n",
    "        pincode=\"\",\n",
    "        rating=rating if rating is not None else \"\",\n",
    "        rating_count=rating_count,\n",
    "        https=(urlparse(url).scheme == \"https\"),\n",
    "        domain_age_years=\"\",\n",
    "        has_policy_pages=True,\n",
    "    )\n",
    "    return row\n",
    "\n",
    "# -------- Main runner --------\n",
    "def build_offers_csv_for_product(inp: dict) -> Path:\n",
    "    # Ensure RUNS_DIR exists (in case Cell 1 wasn’t executed in this kernel)\n",
    "    global RUNS_DIR\n",
    "    if \"RUNS_DIR\" not in globals():\n",
    "        RUNS_DIR = Path.cwd() / \"runs\"\n",
    "        RUNS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    pid = inp[\"product_id\"]\n",
    "    pname = inp[\"product_name\"]\n",
    "    brand = inp.get(\"brand\") or \"\"\n",
    "    query = f\"{pname} {brand}\".strip()\n",
    "\n",
    "    strict_tokens = tokens_from_product(pname, brand)\n",
    "    price_bounds = (30, 2000)  # generous band for this category\n",
    "\n",
    "    # Sites to query\n",
    "    requested = [c.lower() for c in inp.get(\"channels_to_consider\") or []]\n",
    "    site_keys = [k for k in TARGET_SITES.keys() if (not requested or k in requested)]\n",
    "\n",
    "    # 1) Discover URLs per site + filter by product URL rules\n",
    "    candidates = []\n",
    "    for key in site_keys:\n",
    "        domain = TARGET_SITES[key]\n",
    "        found = serper_site_search(query, domain, SERPER_KEY, num=6)\n",
    "        if found:\n",
    "            for u in found:\n",
    "                if is_rejected_url(u): continue\n",
    "                src = source_from_url(u)\n",
    "                if not matches_site_product_rules(u, src): continue\n",
    "                candidates.append(u)\n",
    "        sleep_secs(0.8, 1.5)\n",
    "\n",
    "    # Include manual URLs\n",
    "    if inp.get(\"manual_urls\"):\n",
    "        candidates = list(dict.fromkeys(inp[\"manual_urls\"] + candidates))\n",
    "    else:\n",
    "        candidates = list(dict.fromkeys(candidates))\n",
    "\n",
    "    # 2) Final allowlist by selected channels\n",
    "    allow = set(site_keys)\n",
    "    urls = [u for u in candidates if source_from_url(u) in allow]\n",
    "    if not urls:\n",
    "        raise RuntimeError(\"No candidate product URLs. Add Serper key in Cell 1 or paste Manual URLs, then rerun Cell 2.\")\n",
    "\n",
    "    # 3) Serper shopping quick prices (fallback map)\n",
    "    quick_prices = serper_quick_prices(query, SERPER_KEY)  # domain -> (price, url)\n",
    "\n",
    "    # 4) Scrape with fallback\n",
    "    rows, seen = [], set()\n",
    "    for u in urls:\n",
    "        if u in seen: \n",
    "            continue\n",
    "        seen.add(u)\n",
    "\n",
    "        row = scrape_product_page(u, pname, strict_tokens, price_bounds=price_bounds)\n",
    "        if row:\n",
    "            row[\"delivers_to_location\"] = check_delivery(row[\"source\"], inp[\"location\"])\n",
    "            row[\"legitimacy_score\"] = legitimacy_heuristics(row)\n",
    "            row[\"comparable_price\"] = comparable_price(row)\n",
    "            rows.append(row)\n",
    "            print(f\"[OK] {row['source']}: {row['base_price']} @ {u}\")\n",
    "        else:\n",
    "            # Fallback: use Serper shopping price for the same domain if available\n",
    "            dom = urlparse(u).netloc.lower().replace(\"www.\", \"\")\n",
    "            qp = quick_prices.get(dom)\n",
    "            if qp:\n",
    "                price, qurl = qp\n",
    "                src = source_from_url(qurl)\n",
    "                faux = dict(\n",
    "                    source=src,\n",
    "                    product_title=f\"{pname} (Serper Shopping)\",\n",
    "                    base_price=float(price),\n",
    "                    shipping_fee=0.0, cod_fee=0.0, coupon_value=0.0,\n",
    "                    in_stock=True,\n",
    "                    url=qurl,\n",
    "                    timestamp=datetime.now(timezone.utc).isoformat(),\n",
    "                    pincode=\"\", rating=\"\", rating_count=0,\n",
    "                    https=(urlparse(qurl).scheme == \"https\"),\n",
    "                    domain_age_years=\"\", has_policy_pages=True,\n",
    "                )\n",
    "                faux[\"delivers_to_location\"] = check_delivery(faux[\"source\"], inp[\"location\"])\n",
    "                faux[\"legitimacy_score\"] = legitimacy_heuristics(faux) - 5  # slight penalty for fallback\n",
    "                faux[\"comparable_price\"] = comparable_price(faux)\n",
    "                rows.append(faux)\n",
    "                print(f\"[fallback:shopping] {src}: {price} @ {qurl}\")\n",
    "            else:\n",
    "                print(f\"[skip] {u}\")\n",
    "        sleep_secs(0.9, 2.0)\n",
    "\n",
    "    # 5) Fail if nothing collected\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"No valid offers after parsing. Add Manual URLs and rerun.\")\n",
    "\n",
    "    # 6) Outlier trimming (IQR)\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(rows)\n",
    "    q1, q3 = df[\"base_price\"].quantile(0.25), df[\"base_price\"].quantile(0.75)\n",
    "    iqr = max(1.0, q3 - q1)\n",
    "    low, high = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    df = df[(df[\"base_price\"] >= max(price_bounds[0], low)) & (df[\"base_price\"] <= min(price_bounds[1], high))]\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"All parsed prices looked like outliers. Add Manual URLs or widen price bounds.\")\n",
    "\n",
    "    # 7) Write CSV\n",
    "    out_dir = RUNS_DIR / pid\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    out_csv = out_dir / f\"{pid}_offers.csv\"\n",
    "\n",
    "    for col in [\"base_price\",\"shipping_fee\",\"cod_fee\",\"coupon_value\",\"rating\",\"rating_count\",\n",
    "                \"legitimacy_score\",\"comparable_price\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"\\nWrote {len(df)} rows to {out_csv}\")\n",
    "    return out_csv\n",
    "\n",
    "# ---- Execute for THIS product ----\n",
    "offers_csv_path = build_offers_csv_for_product(USER_INPUT)\n",
    "offers_csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486061f-7716-47a6-b8a0-9a06cee40394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78210f51-11ec-4dd4-8408-07130b8334c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
